---
title: An Empirical Analysis of Off-policy Learning in Discrete MDPs
abstract: Off-policy evaluation is the problem of evaluating a decision-making policy
  using data collected under a different behaviour policy. While several methods are
  available for addressing off-policy evaluation, little work has been done on identifying
  the best methods. In this paper, we conduct an in-depth comparative study of several
  off-policy evaluation methods in non-bandit, finite-horizon MDPs, using randomly
  generated MDPs, as well as a Mallard population dynamics model [Anderson, 1975]
  . We find that un-normalized importance sampling can exhibit prohibitively large
  variance in problems involving look-ahead longer than a few time steps, and that
  dynamic programming methods perform better than Monte-Carlo style methods.
pdf: http://proceedings.pmlr.press/paduraru12a/paduraru12a.pdf
layout: inproceedings
id: paduraru12a
month: 0
firstpage: 89
lastpage: 102
page: 89-102
origpdf: http://jmlr.org/proceedings/papers/v24/paduraru12a/paduraru12a.pdf
sections: 
author:
- given: Cosmin
  family: "PÄ\x83duraru"
- given: Doina
  family: Precup
- given: Joelle
  family: Pineau
- given: Gheorghe
  family: "ComÄ\x83nici"
date: 2013-01-12
publisher: PMLR
container-title: Proceedings of the Tenth European Workshop on Reinforcement Learning
volume: '24'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 1
  - 12
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
