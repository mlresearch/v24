---
title: Actor-Critic Reinforcement Learning with Energy-Based Policies
abstract: We consider reinforcement learning in Markov decision processes with high
  dimensional state and action spaces. We parametrize policies using energy-based
  models (particularly restricted Boltzmann machines), and train them using policy
  gradient learning. Our approach builds upon Sallans and Hinton (2004), who parameterized
  value functions using energy-based models, trained using a non-linear variant of
  temporal-difference (TD) learning. Unfortunately, non-linear TD is known to diverge
  in theory and practice. We introduce the first sound and efficient algorithm for
  training energy-based policies, based on an actor-critic architecture. Our algorithm
  is computationally efficient, converges close to a local optimum, and outperforms
  Sallans and Hinton (2004) in several high dimensional domains.
pdf: http://proceedings.mlr.press/v24/heess12a/heess12a.pdf
layout: inproceedings
id: heess12a
month: 0
firstpage: 45
lastpage: 58
page: 45-58
sections: 
author:
- given: Nicolas
  family: Heess
- given: David
  family: Silver
- given: Yee Whye
  family: Teh
date: 2013-01-12
address: Edinburgh, Scotland
publisher: PMLR
container-title: Proceedings of the Tenth European Workshop on Reinforcement Learning
volume: '24'
genre: inproceedings
issued:
  date-parts:
  - 2013
  - 1
  - 12
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
