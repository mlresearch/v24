---
title: Directed Exploration in Reinforcement Learning with Transferred Knowledge
abstract: "Experimental results suggest that transfer learning (TL), compared to learning
  from scratch, can decrease exploration by reinforcement learning (RL) algorithms.
  Most existing TL algorithms for RL are heuristic and may result in worse performance
  than learning from scratch (i.e., negative transfer). We introduce a theoretically
  grounded and flexible approach that transfers action-values via an intertask mapping
  and, based on those, explores the target task systematically. We characterize positive
  transfer as (1) decreasing sample complexity in the target task compared to the
  sample complexity of the base RL algorithm (without transferred action-values) and
  (2) guaranteeing that the algorithm converges to a near-optimal policy (i.e., negligible
  optimality loss). The sample complexity of our approach is no worse than the base
  algorithm√¢\x80\x99s, and our analysis reveals that positive transfer can occur even
  with highly inaccurate and partial intertask mappings. Finally, we empirically test
  directed exploration with transfer in a multijoint reaching task, which highlights
  the value of our analysis and the robustness of our approach under imperfect conditions."
pdf: "./mann12a/mann12a.pdf"
layout: inproceedings
key: mann12a
month: 0
firstpage: 59
lastpage: 76
origpdf: http://jmlr.org/proceedings/papers/v24/mann12a/mann12a.pdf
sections: 
authors:
- given: Timothy A.
  family: Mann
- given: Yoonsuck
  family: Choe
---
